# ğŸ› Rover Vision-Control Project (Raspberry Pi + YOLO + Gesture Control)

This project enables real-time control of a ground rover using hand gestures and object/person tracking via computer vision. The system follows a **client-server architecture** optimized for Raspberry Pi Zero 2W.

---

## ğŸš¦ Project Overview

- **Client (Raspberry Pi Zero 2W)**: Captures camera feed and sends frames over WebSocket.
- **Server (MacBook or PC)**: Performs gesture detection, object/person tracking, and sends back movement commands.
- **Communication**: Lightweight WebSocket protocol over local Wi-Fi.

---

## ğŸ§  Functional Modes

| Mode               | Trigger Gesture       | Description                                    |
|--------------------|------------------------|------------------------------------------------|
| Idle Mode          | No gesture detected    | Rover stops and awaits input                  |
| Hand Tracking Mode | Open palm (ğŸ–ï¸)         | Tracks hand movement to control direction     |
| Command Mode       | Thumbs-up, peace, etc. | Executes predefined commands (e.g., stop)     |
| Person Tracking    | Pointing gesture       | Tracks selected person from YOLO detection    |

---

## ğŸ–¥ï¸ Server-Side (MacBook/PC)

### ğŸ”§ Responsibilities
- Run YOLO (hand/person detection)
- Run gesture recognition (MediaPipe or custom CNN)
- FSM-based control logic (mode switching)
- Send commands to client over WebSocket

### ğŸ› ï¸ Tools & Libraries
- Python 3.10+
- OpenCV
- YOLOv8 (Ultralytics)
- MediaPipe or custom hand gesture model
- WebSocket server: `websockets` or `FastAPI + WebSocket`

---

## ğŸ“ Client-Side (Raspberry Pi Zero 2W)

### ğŸ”§ Responsibilities
- Capture camera feed (PiCamera/OpenCV)
- Compress and transmit frames to server
- Receive motor commands (JSON)
- Drive GPIO-controlled motors

### ğŸ› ï¸ Tools & Libraries
- Python 3.9+ (Lite OS)
- `opencv-python` (use headless version)
- `websockets` or `websocket-client`
- `RPi.GPIO` or `gpiozero`

---

## ğŸ—‚ï¸ Suggested File Structure
```
rover_project/
â”œâ”€â”€ client/ # Code for Raspberry Pi
â”‚ â”œâ”€â”€ camera_streamer.py # Captures video and sends frames
â”‚ â”œâ”€â”€ motor_controller.py # Executes commands via GPIO
â”‚ â”œâ”€â”€ client_main.py # WebSocket client loop
â”‚ â””â”€â”€ utils/ # Frame compression, logging
â”œâ”€â”€ server/ # Code for YOLO and control logic
â”‚ â”œâ”€â”€ gesture_detector.py # Detects hand gestures
â”‚ â”œâ”€â”€ object_tracker.py # Person/hand tracking module
â”‚ â”œâ”€â”€ fsm_controller.py # FSM mode switch logic
â”‚ â”œâ”€â”€ server_main.py # WebSocket server loop
â”‚ â””â”€â”€ models/ # Pretrained YOLO or CNN weights
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ requirements.txt # Shared dependencies
```


---

## âœ… Next Steps

- [ ] Implement YOLO + gesture detector (server side)
- [ ] Setup camera and WebSocket communication (client side)
- [ ] Build FSM controller (hand â†’ command â†’ GPIO)
- [ ] Test latency and optimize image transmission
- [ ] Expand modes: person-follow, auto-stop on lost signal, etc.

---

## ğŸ“Œ Notes

- Raspberry Pi Zero 2W lacks GPU, hence no onboard vision processing
- Real-time inference is handled entirely by the server
- System designed to run over low-latency Wi-Fi or local hotspot

---

## ğŸ”— References

- YOLOv8: https://docs.ultralytics.com/
- MediaPipe Hands: https://google.github.io/mediapipe/solutions/hands
- FastAPI WebSocket: https://fastapi.tiangolo.com/advanced/websockets/


## Architecture

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚       FastAPI Client        â”‚ (Health Check, Manual Triggers)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     CommunicationManager    â”‚ <-- Singleton (handles all WebSocket I/O)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²               â–²
         â”‚               â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Sensor     â”‚   â”‚   Executor   â”‚
 â”‚  Modules     â”‚   â”‚   Modules    â”‚
 â”‚ (Camera etc) â”‚   â”‚ (Motor etc)  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```


## Folder Structure

```
rover_client/
â”‚
â”œâ”€â”€ rover_client.py              # ğŸš€ FastAPI entry point and orchestrator
â”œâ”€â”€ config.yaml                  # âš™ï¸  Global config (URIs, flags, etc.)
â”œâ”€â”€ communication_manager.py     # ğŸ” Singleton for all WebSocket I/O and routing
â”‚
â”œâ”€â”€ sensors/
â”‚   â”œâ”€â”€ sensor.py                # ğŸ“¡ Abstract base class for all sensors
â”‚   â””â”€â”€ camera_sensor.py         # ğŸ“· Captures frames and streams via CommunicationManager
â”‚
â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ controller.py            # ğŸ® Base class for executors
â”‚   â””â”€â”€ motor_controller.py      # âš™ï¸  Executes motor commands (includes low-level GPIO ops)
â”‚
â”œâ”€â”€ utils.py                     # ğŸ§° Shared utilities (logging, config loading, etc.)
```
